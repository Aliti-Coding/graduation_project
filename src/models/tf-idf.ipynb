{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing TF-IDF\n",
    "1. documentaion: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv file\n",
    "df_read = pd.read_csv(\"Books_5_partition_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    \"\"\"Removes html tags, everything except letters, \n",
    "    and filters out stop words\n",
    "    example how to use function: \\n \n",
    "    for i in range(0, len(train['review'])): \\n\n",
    "        clean_train_reviews.append(review_to_words(train['review'][i])).\n",
    "\n",
    "    Args:\n",
    "        raw_review (_str_): _input the column you want to transform_\n",
    "\n",
    "    Returns:\n",
    "        _str_: _a string that is transformed_\n",
    "    \"\"\"\n",
    "    #1 if any html tags, removed \n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "\n",
    "    #2 remove puctions and numbers\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "\n",
    "    #3 convert to lowercase and split\n",
    "    words_lst = letters_only.lower().split()\n",
    "\n",
    "    #4 convert stop words to set for increased speed processing\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    #5 remove stop words from the text\n",
    "    meaningful_words = [w for w in words_lst if not w in stops] #if w in stops remove it\n",
    "\n",
    "    #6 transform the list to text string\n",
    "    meaningful_words_str = \" \".join(meaningful_words)\n",
    "\n",
    "    return meaningful_words_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filepath):\n",
    "    \"\"\" reads csv file to a dataframe and cleans review column\n",
    "\n",
    "    Args:\n",
    "        filepath (_str_): _filepath of the csv file_\n",
    "\n",
    "    Returns:\n",
    "        _list_: _returns a list with clean text with no stopwords_\n",
    "    \"\"\"\n",
    "    df_orig = pd.read_csv(filepath)\n",
    "\n",
    "    clean_review = []\n",
    "\n",
    "    for line in range(0, len(df_orig['reviewText'])):\n",
    "        clean_review.append(review_to_words(df_orig['reviewText'][line]))\n",
    "    return clean_review\n",
    "clean_data(\"Books_5_partition_1.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding how many different words there are in the corpus\n",
    "def words_in_corpus(clean_text):\n",
    "    \"\"\"finding how many different words there are in the corpus\n",
    "    Args:\n",
    "        clean_text (_list_): _clean-reviews_\n",
    "    Returns:\n",
    "        _str_: _number of unique words in the corpus_\n",
    "    \"\"\"     \n",
    "\n",
    "    words_set = set()\n",
    "    for i in clean_text:\n",
    "        words = i.split(' ')\n",
    "        # print(words)\n",
    "        words_set = words_set.union(set(words))\n",
    "\n",
    "    return f'number of words in the corpus {len(words_set)}'\n",
    "\n",
    "words_in_corpus(clean_data(\"Books_5_partition_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features= 1000, # Selects most frequent words in the corpus when computing the TF-IDF. useful for performance if you have large datasets\n",
    "    # max_df=  0.8, # removes words that appears 80% in the text.\n",
    "    min_df = 5, # removes word that appears less than 5 times\n",
    "    ngram_range= (1,3), #is range to capture the conext and meaning of words. means it checks 3 words at a time.\n",
    "    get_params=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(clean_data(\"Books_5_partition_1.csv\")) #use the function we made above to get clean data \n",
    "# print(vectors)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out() #feature names that are most frequent. you can changes this in the max_feature parameter when using TfidfVectorizer\n",
    "# print(feature_names)\n",
    "\n",
    "dense = vectors.toarray() # returns a sparse matrix with shape (rows * feature_names)\n",
    "# print(dense)\n",
    "\n",
    "denselist = dense.tolist()\n",
    "\n",
    "# print(\"vocubulary:\", vectorizer.vocabulary_) #prints a dictionary counting number of times a feature appears"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TF-IDF scores sorted\n",
    "##### extract the words with highest tf-idf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top words \n",
    "def top_words(top_rows):\n",
    "    \"\"\" Returns a DF with sorted tf-idf score\n",
    "    Args:\n",
    "        select_range (_int_): _select top rows you want_\n",
    "    Returns:\n",
    "        _DataFrame_: _Returns a DataFrame with the top rows selected_\n",
    "    \"\"\"\n",
    "    #sum up the number of each vocabulary word.\n",
    "    dist = np.sum(dense, axis=0) #axis 0 to sum all the rows\n",
    "\n",
    "    word_count = []\n",
    "    for tag, count in zip(feature_names, dist):\n",
    "        word_count.append((count, tag))\n",
    "\n",
    "    df_word_count = pd.DataFrame(word_count, columns=[\"tf-idf-score\", \"word\"])\n",
    "\n",
    "    df_sorted = df_word_count.sort_values(by=['tf-idf-score'], ascending=False)\n",
    "    \n",
    "    return df_sorted[:top_rows]\n",
    "\n",
    "top_words(top_rows=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koden under er treg og må forbedres \n",
    "men per nå funker den som den er "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop goes through every line that is tf-idf'ed and extracts values above 0,\n",
    "# wich means words that have occured inside our chosen vocabulary\n",
    "# in the end we get words that is in our vocubulary and in the review\n",
    "all_keywords = []\n",
    "hottest_word = [] #stores the word with highest tf-idf score for the document\n",
    "for description in denselist:\n",
    "    x=0\n",
    "    keywords = []\n",
    "    hottest_key = []\n",
    "    hottested_word = max(description)\n",
    "   \n",
    "    \n",
    "    for word in description:\n",
    "        if word > 0:\n",
    "            keywords.append(feature_names[x])\n",
    "        \n",
    "        if word == hottested_word:\n",
    "            hottest_key.append(feature_names[x])\n",
    "\n",
    "        x=x+1\n",
    "  \n",
    "    hottest_word.append(hottest_key)\n",
    "    all_keywords.append(keywords)\n",
    "\n",
    "# print(hottest_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eksempel med numpy, ikke helt ferdig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_indices  = np.nonzero(dense)\n",
    "# print(no_zeros[0])\n",
    "\n",
    "np_all_keywords = [feature_names[i] for i in nonzero_indices[1]]\n",
    "print(np_all_keywords)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hottest_word_indices = np.argmax(dense, axis=1)\n",
    "print(hottest_word_indices)\n",
    "#gives you the hottest word\n",
    "np_hottest_word = [feature_names[index] for index in hottest_word_indices]\n",
    "print(len(np_hottest_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_keywords))\n",
    "print(len(df_read['reviewText']))\n",
    "print(len(hottest_word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "we get a column with the review text, and the tf-idf words and the hottest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rev_tf_idf = []\n",
    "for raw, tf_idf, hottest_w in zip(df_read['reviewText'], all_keywords, hottest_word):\n",
    "    list_rev_tf_idf.append((raw, tf_idf, hottest_w))\n",
    "\n",
    "\n",
    "\n",
    "df_tf_idf = pd.DataFrame(list_rev_tf_idf, columns=['reviewText', 'tf-idf-summary', 'hottest_word'])\n",
    "# test = df_tf_idf[df_tf_idf['tf-idf-summary'] != None]\n",
    "\n",
    "# print(df_tf_idf[df_tf_idf['hottest_word'] == 'able,action,actually,adult,adults,adventure,adventures,age,ago,almost,along,already,also,although,always,amazon,american,animals,another,anyone,anything,around,aslan,author,authors,away,back,bad,based,battle,beautiful,become,becomes,beginning,behavior,believe,bell,best,better,big,bit,book,book read,books,boring,bought,boy,buy,called,came,care,case,caspian,cat,certainly,chapter,chapters,character,character development,characters,chee,child,children,christian,chronicles,chronicles narnia,classic,clear,come,comes,complete,completely,computer,copy,could,couple,course,cover,crichton,daughter,dawn,dawn treader,day,days,death,decided,definitely,description,desert,development,dialogue,different,difficult,digory,disappointed,done,dr,dr seuss,easy,edition,edmund,either,else,end,ending,english,enjoy,enjoyable,enjoyed,enough,entertaining,entire,especially,etc,eustace,even,events,ever,every,everyone,everything,evil,example,excellent,except,expect,expected,fact,family,fan,fantasy,far,fast,favorite,feel,felt,fiction,finally,find,finish,finished,first,follow,form,found,four,friend,friends,full,fun,gave,get,gets,getting,gift,girl,give,given,giving,go,god,goes,going,gone,good,good book,got,great,green,group,guess,guy,half,hand,happen,happened,happy,hard,help,hemingway,hero,high,highly,hillerman,historical,history,home,hope,however,human,idea,ideas,illustrations,important,information,instead,interest,interested,interesting,jack,job,jordan,julia,jurassic,jurassic park,keep,kept,kid,kids,kind,kindle,king,know,land,language,large,last,last battle,later,leaphorn,learn,least,left,less,let,level,lewis,library,life,like,liked,line,lion,lion witch,lion witch wardrobe,literature,little,live,lives,long,look,looking,lost,lot,love,loved,lucy,made,magic,magical,magician,magician nephew,main,main character,make,makes,making,man,management,manager,many,may,maybe,mean,message,michael,michael crichton,middle,might,mind,minute,modern,money,movie,mr,much,must,mystery,name,nano,nanotechnology,narnia,narrative,nature,need,nephew,never,new,next,nice,nothing,novel,novels,obvious,often,oh,ok,old,older,one,one minute,opinion,order,original,others,overall,page,pages,parents,park,part,particular,particularly,parts,past,people,perhaps,person,peter,pick,picture,pictures,place,plot,point,polly,present,pretty,prey,prince,prince caspian,probably,problem,problems,protagonist,published,put,quality,quickly,quite,rather,read,read book,reader,readers,reading,reading book,reads,real,really,reason,recommend,remember,research,rest,return,review,reviews,right,robert,robert jordan,said,save,saw,say,says,scene,scenes,school,science,scientific,second,see,seem,seemed,seems,seen,self,sense,series,set,setting,seuss,seven,several,short,side,simple,simply,since,slow,small,someone,something,sometimes,somewhat,son,soon,spanish,star,stars,start,started,starts,stay,still,stories,story,strange,stupid,style,sure,susan,suspense,swarm,take,takes,tale,talk,talking,technical,technology,tell,tells,text,th,thing,things,think,thinking,third,though,thought,three,throughout,time,timeline,times,title,today,together,told,took,totally,treader,true,truly,try,trying,turn,two,type,uncle,understand,unfortunately,us,use,used,using,usual,version,want,wanted,war,wardrobe,way,ways,well,well written,went,whole,wife,wish,witch,witch wardrobe,within,without,woman,wonder,wonderful,word,words,work,works,world,worlds,worth,would,write,writer,writing,written,wrong,wrote,year,year old,years,yes,yet'])\n",
    "# test[test['reviewText'] == 'thanks!']\n",
    "df_tf_idf.dtypes\n",
    "\n",
    "# df_tf_idf_filtered.to_csv('controllingdata.csv')\n",
    "\n",
    "# print(df_tf_idf['reviewText'][0])\n",
    "# print()\n",
    "# print(df_tf_idf['tf-idf-summary'][0])\n",
    "# print()\n",
    "# print(df_tf_idf['hottest_word'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering (stretch)\n",
    "##### maybe it can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 20\n",
    "\n",
    "model = KMeans(\n",
    "    n_clusters=true_k, \n",
    "    init=\"k-means++\", \n",
    "    max_iter= 100\n",
    "    )\n",
    "model.fit(vectors)\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "print(order_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            f.write(' %s' % feature_names[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bebc719c1a5129d18b92cf73783d66f7154b28203397ec213f9ee16fb76f3f07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
